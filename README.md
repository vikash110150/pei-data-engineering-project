# PEI Data Engineering Project

### Project Overview
This project implements a comprehensive data engineering solution for processing e-commerce sales data using Databricks, PySpark, and the Medallion Architecture pattern. The solution handles multiple data formats (CSV, JSON, XLSX) and provides robust data transformations with comprehensive unit testing.
---

## ğŸ—ï¸ Architecture

### Medallion Architecture (Bronze â†’ Silver â†’ Gold)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES                             â”‚
â”‚  JSON (Orders)  â”‚  CSV (Products)  â”‚  XLSX (Customer)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                    â”‚
                 â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BRONZE LAYER (Raw Data)                    â”‚
â”‚  â€¢ bronze_orders   â€¢ bronze_products   â€¢ bronze_customers       â”‚
â”‚  â€¢ No transformations  â€¢ Data as-is from source                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                    â”‚
                 â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SILVER LAYER (Cleansed Data)                  â”‚
â”‚  â€¢ silver_customers                                             â”‚
â”‚  â€¢ silver_products                                              â”‚
â”‚  â€¢ silver_orders                                                â”‚
â”‚                                                                 â”‚
â”‚  âœ” Standardized schemas                                         â”‚
â”‚  âœ” Data quality checks                                          â”‚
â”‚  âœ” Cleaned + validated fields                                   â”‚
â”‚  âœ” Duplication removed                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                       â”‚
                        â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GOLD LAYER (Curated)                    â”‚
â”‚                                                                 â”‚
â”‚ 1 gold_orders                                                   â”‚
â”‚     â€¢ Enriched fact table combining orders + customers +productsâ”‚
â”‚     â€¢ Profit rounded to 2 decimals                              â”‚
â”‚     â€¢ Cleaned + analytics-ready                                 â”‚
â”‚     â€¢ Includes processing timestamp                             â”‚
â”‚                                                                 â”‚
â”‚ 2 gold_profit                                                   â”‚
â”‚     â€¢ Yearly aggregated profit                                  â”‚
â”‚     â€¢ Grouped by: Year, Customer, Category, Sub-Category        â”‚
â”‚     â€¢ Metrics: SUM(profit), COUNT(order_id)                     â”‚
â”‚     â€¢ All profit values rounded to 2 decimals                   â”‚
â”‚                                                                 â”‚
â”‚ 3  SQL Aggregates (Materialized Tables)                         â”‚
â”‚     â€¢ profit_by_year                                            â”‚
â”‚     â€¢ profit_by_year_category                                   â”‚
â”‚     â€¢ profit_by_customer                                        â”‚
â”‚     â€¢ profit_by_customer_year                                   â”‚
â”‚                                                                 â”‚
â”‚  âœ” Business-ready data                                          â”‚
â”‚  âœ” Optimized for dashboards & reporting                         â”‚
â”‚  âœ” Delivered as Delta tables (ACID, versioned)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

##  Project Structure

```

```
pei-data-engineering-project/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                    # Configuration file
â”‚
â”œâ”€â”€ src/                                # Source code modules
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ ingestion.py               # Bronze layer ingestion logic
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ transformation.py          # Silver layer transformation logic
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ aggregation.py             # Gold layer aggregation logic
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ string_cleaners.py         # Helper functions
â”‚
â”œâ”€â”€ tests/                             # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                    # Pytest configuration
â”‚   â”œâ”€â”€ test_ingestion.py              # Bronze layer tests
â”‚   â”œâ”€â”€ test_transformation.py         # Silver layer tests
â”‚   â””â”€â”€ test_aggregation.py            # Gold layer tests
â”‚
â”œâ”€â”€ main()                             # End-to-end pipeline runner (Bronze â†’ Silver â†’ Gold)
â”œâ”€â”€ run_pytests                        # Notebook to execute all Pytest tests
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                           # Sample/test data
â”‚
â”œâ”€â”€ libs/                              # Python library to read Excel file
â”‚
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ setup.py                           # Package setup
â”œâ”€â”€ pytest.ini                         # Pytest configuration
â”œâ”€â”€ README.md                          # This file
â”‚
â””â”€â”€ notebooks/                         # Databricks notebooks
    â”œâ”€â”€ 01_Data_Ingestion.py           # Test Bronze ingestion
    â”œâ”€â”€ 02_Data_Transformation.py      # Test Silver transformation
    â”œâ”€â”€ 03_Data_Aggregation.py         # Test Gold aggregation
    â””â”€â”€ 04_Analytics_Queries.py        # Test SQL analytics queries
```

```

The entire project is version-controlled using Git.

Repository Structure:
â€¢ Git folder created to store all source code, notebooks, and configs
â€¢ Code is committed and pushed regularly for version tracking
â€¢ Suitable for collaborative development and CI/CD workflows

Pipeline Overview â€” pei-data-engineering-pipeline
This pipeline runs the complete end-to-end data flow:
Raw Data â†’ Bronze â†’ Silver â†’ Gold â†’ pytest
```

```
## ğŸš€ Getting Started

### Prerequisites
- Databricks workspace
- Databricks Runtime 12.2 LTS or higher
- Python 3.9+
- Access to Google Drive data source

### Installation Steps

#### 1. Download Data from Google Drive
# Downloaded the datasets from:
# https://drive.google.com/drive/folders/1eWxfGcFwJJKAK0Nj4zZeCVx6gagPEEVc?usp=sharing

# Expected files:
# - orders.json
# - products.csv
# - customers.xlsx
```

```
#### 3. Upload Data Files

1. In Databricks, open the left sidebar.
2. Go to **Workspace** â†’ navigate to your project directory.
3. Right-click and create a folder named: raw_data 
4. Right-click the raw_data folder â†’ **Upload**
5. Upload the 3 source files:

   â€¢ orders.json  
   â€¢ products.csv  
   â€¢ customers.xlsx  



```
**Python Libraries**
```python
# Run in notebook
%pip install pandas openpyxl pytest pyyaml
```

#### 5. Update Configuration

Edit `config/config.yaml` with your paths:
```yaml
data_paths:
  raw_base_path: "/Workspace/Users/vikash110150@gmail.com/de_project/raw_data/raw"

tables:
  bronze:
    orders: "workspace.default.bronze_orders"
    products: "workspace.default.bronze_products"
    customers: "workspace.default.bronze_customers"

  silver:
    orders: "workspace.default.silver_orders"
    products: "workspace.default.silver_products"
    customers: "workspace.default.silver_customers"

  gold:
    orders: "workspace.default.gold_orders"
    profit: "workspace.default.gold_profit"

source_files:
  orders: "Orders.json"
  products: "Products.csv"
  customers: "Customer.xlsx"
```


## How to Run the Project
Pipeline Overview â€” pei-data-engineering-pipeline
The pipeline runs automatically (scheduled job)
It can also be triggered manually from the Databricks UI
Databricks Workspace â†’ Jobs â†’ pei-data-engineering-pipeline â†’ Run Now
This pipeline runs the complete end-to-end data flow:

## 1. Run the Entire Pipeline (`main()`)
The `main()` notebook/script runs the full ETL pipeline:
- Bronze ingestion
- Silver cleansing
- Gold aggregation
- SQL materialization
 **Path:** `main()`
### 2. Run All Tests (`run_pytests`)
The `run_pytests` notebook automatically discovers and executes all tests located under the `tests/` folder using Pytest.

 **Path:** `run_pytests`

**Test Coverage:**
- Bronze layer ingestion tests
- Silver layer transformation tests
- Gold layer aggregation tests
- File converter tests
- Data quality validation tests

---

## ğŸ“‹ Task Requirements Implementation

### âœ… Task 1: Create raw tables for each source dataset
- **Implemented in:** `ingestion.py`
- **Tables:** `bronze_orders`, `bronze_products`, `bronze_customers`
- **Format:** Delta tables with full schema inference

### âœ… Task 2: Create enriched table for customers and products
- **Implemented in:** `transformation.py`
- **Tables:** `silver_customers_enriched`, `silver_products_enriched`
- **Features:** Data cleansing, deduplication, schema standardization

### âœ… Task 3: Create enriched table with order information


### âœ… Task 4: Create aggregate table showing profit by dimensions
Implemented in: aggregation.py
Task 1: Create enriched Gold Orders table
Includes:
Orders joined with customers + products
Profit rounded to 2 decimals
Customer name, country
Product category, sub-category
Task 2: Create aggregated gold profit table
Dimensions:Year,Customer,Category,Sub-Category
Metrics:
Total Profit (rounded)
Total Orders
Task 3: Generate SQL-based analytics tables
Implemented in: aggregation.py
Tables:
profit_by_year
profit_by_year_category
profit_by_customer
profit_by_customer_year


## ğŸ›¡ï¸ Data Quality & Error Handling

### Implemented Checks:
1. **Schema Validation:** Ensures correct data types and required columns
2. **Duplicate Detection:** Removes duplicate records
3. **Data Type Conversion:** Proper casting of numeric and date fields
4. **Business Rule Validation:** Profit calculation validation

### Error Handling:
- Try-catch blocks for file operations
- Logging of errors and warnings
- Graceful degradation for missing data

---

## ğŸ“ Assumptions & Design Decisions

1. **Date Format:** Order dates assumed to be in ISO format (YYYY-MM-DD)
2. **Deduplication:** Based on primary keys (order_id, customer_id, product_id)
3. **Year Extraction:** From order_date field
4. **Schema Evolution:** Enabled for all Delta tables

---


## ğŸ“ Support & Contact

For questions or issues:
- Create an issue in the project repository
- Contact: vikash110150@gmail.com
- Documentation: [Databricks Documentation](https://docs.databricks.com/)

---

## ğŸ“„ License

This project is created for educational and assessment purposes.

---

---

**Happy Data Engineering!
