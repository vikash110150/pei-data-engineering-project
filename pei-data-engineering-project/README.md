# PEI Data Engineering Project
## E-commerce Sales Data Processing with Databricks

### Project Overview
This project implements a comprehensive data engineering solution for processing e-commerce sales data using Databricks, PySpark, and the Medallion Architecture pattern. The solution handles multiple data formats (CSV, JSON, XLSX) and provides robust data transformations with comprehensive unit testing.

---

## ğŸ—ï¸ Architecture

### Medallion Architecture (Bronze â†’ Silver â†’ Gold)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES                              â”‚
â”‚  CSV (Orders)  â”‚  JSON (Products)  â”‚  XLSX (Customers)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                    â”‚
                 â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BRONZE LAYER (Raw Data)                     â”‚
â”‚  â€¢ bronze_orders   â€¢ bronze_products   â€¢ bronze_customers        â”‚
â”‚  â€¢ No transformations  â€¢ Data as-is from source                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                    â”‚
                 â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SILVER LAYER (Cleansed Data)                   â”‚
â”‚  â€¢ silver_customers                                              â”‚
â”‚  â€¢ silver_products                                               â”‚
â”‚  â€¢ silver_orders(with profit, customer, product info)            â”‚
â”‚  â€¢ Data quality checks  â€¢ Deduplication  â€¢ Schema enforcement    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                    â”‚
                 â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GOLD LAYER (Aggregated)                      â”‚
â”‚  â€¢ gold_profit_aggregates (Year, Category, SubCategory, Customer)â”‚
â”‚  â€¢ Business-ready data  â€¢ Optimized for analytics                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
pei-data-engineering-project/
â”‚
â”œâ”€â”€ notebooks/                          # Databricks notebooks
â”‚   â”œâ”€â”€ 01_Data_Ingestion.py           # Bronze layer - data ingestion
â”‚   â”œâ”€â”€ 02_Data_Transformation.py      # Silver layer - enrichment
â”‚   â”œâ”€â”€ 03_Data_Aggregation.py         # Gold layer - aggregations
â”‚   â””â”€â”€ 04_Analytics_Queries.py        # SQL analytics queries
â”‚
â”œâ”€â”€ src/                                # Source code modules
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ ingestion.py               # Bronze layer ingestion logic
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ transformation.py          # Silver layer transformation logic
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ aggregation.py             # Gold layer aggregation logic
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py                  # Configuration management
â”‚       â”œâ”€â”€ file_converter.py          # XLSX to CSV converter
â”‚       â””â”€â”€ spark_utils.py             # Spark utility functions
â”‚
â”œâ”€â”€ tests/                              # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                    # Pytest configuration
â”‚   â”œâ”€â”€ test_ingestion.py          # Bronze layer tests
â”‚   â”œâ”€â”€ test_transformation.py     # Silver layer tests
â”‚   â”œâ”€â”€ test_aggregation.py        # Gold layer tests
â”‚   â””â”€â”€ test_file_converter.py     # File converter tests
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                    # Configuration file
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                           # Sample/test data
â”‚
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ setup.py                           # Package setup
â”œâ”€â”€ pytest.ini                         # Pytest configuration
â””â”€â”€ README.md                          # This file
```

---

## ğŸš€ Getting Started

### Prerequisites
- Databricks workspace
- Databricks Runtime 12.2 LTS or higher
- Python 3.9+
- Access to Google Drive data source

### Installation Steps

#### 1. Download Data from Google Drive
```bash
# Download the datasets from:
# https://drive.google.com/drive/folders/1eWxfGcFwJJKAK0Nj4zZeCVx6gagPEEVc?usp=sharing

# Expected files:
# - orders.csv
# - products.json
# - customers.xlsx
```


#### 3. Upload Data Files

**Method 1: DBFS File Upload (UI)**
```
1. Go to Databricks workspace
2. Click 'Data' in the left sidebar
3. Click 'DBFS' â†’ 'Upload'
4. Upload your files to: /FileStore/pei-data-engineering/raw/
```


**Python Libraries**
```python
# Run in notebook
%pip install pandas openpyxl pytest pyyaml
```

#### 5. Update Configuration

Edit `config/config.yaml` with your paths:
```yaml
data_paths:
  raw_base_path: "/Workspace/Users/vikash110150@gmail.com/raw_data/raw"

tables:
  bronze:
    orders: "workspace.default.bronze_orders"
    products: "workspace.default.bronze_products"
    customers: "workspace.default.bronze_customers"

  silver:
    orders: "workspace.default.silver_orders"
    products: "workspace.default.silver_products"
    customers: "workspace.default.silver_customers"

  gold:
    orders: "workspace.default.gold_orders"
    profit: "workspace.default.gold_profit"

source_files:
  orders: "Orders.json"
  products: "Products.csv"
  customers: "Customer.xlsx"
```

---

## Data Pipeline Execution

### Step-by-Step Execution


#### Step 2: Bronze Layer - Data Ingestion
```python
# Run: notebooks/01_Data_Ingestion.py
# Creates: bronze_orders, bronze_products, bronze_customers
```

#### Step 3: Silver Layer - Data Transformation
```python
# Run: notebooks/02_Data_Transformation.py
# Creates: silver_customers_enriched, silver_products_enriched, silver_orders_enriched
```

#### Step 4: Gold Layer - Data Aggregation
```python
# Run: notebooks/03_Data_Aggregation.py
# Creates: gold_profit_aggregates
```

#### Step 5: Analytics Queries
```python
# Run: notebooks/04_Analytics_Queries.py
# Generates: Profit by Year, by Category, by Customer, etc.
```

---

## ğŸ§ª Testing

### Running Unit Tests

**In Databricks Notebook:**
```python
# Install pytest
%pip install pytest

# Run all tests
!pytest /Workspace/Users/vikash110150@gmail.com/pei-data-engineering-project/tests/ -v

# Run specific test module
!pytest /Workspace/Users/vikash110150@gmail.com/pei-data-engineering-project/tests/unit/test_ingestion.py -v

# Run with coverage
!pytest /Workspace/Users/vikash110150@gmail.com/pei-data-engineering-project/tests/ --cov=src --cov-report=html
```

**Test Coverage:**
- Bronze layer ingestion tests
- Silver layer transformation tests
- Gold layer aggregation tests
- File converter tests
- Data quality validation tests

---

## ğŸ“‹ Task Requirements Implementation

### âœ… Task 1: Create raw tables for each source dataset
- **Implemented in:** `notebooks/01_Data_Ingestion.py`
- **Tables:** `bronze_orders`, `bronze_products`, `bronze_customers`
- **Format:** Delta tables with full schema inference

### âœ… Task 2: Create enriched table for customers and products
- **Implemented in:** `notebooks/02_Data_Transformation.py`
- **Tables:** `silver_customers_enriched`, `silver_products_enriched`
- **Features:** Data cleansing, deduplication, schema standardization

### âœ… Task 3: Create enriched table with order information
- **Implemented in:** `notebooks/02_Data_Transformation.py`
- **Table:** `silver_orders_enriched`
- **Includes:**
  - Order information with profit (rounded to 2 decimals)
  - Customer name and country
  - Product category and sub-category

### âœ… Task 4: Create aggregate table showing profit by dimensions
- **Implemented in:** `notebooks/03_Data_Aggregation.py`
- **Table:** `gold_profit_aggregates`
- **Dimensions:** Year, Product Category, Product Sub-Category, Customer

### âœ… Task 5: SQL aggregates
- **Implemented in:** `notebooks/04_Analytics_Queries.py`
- **Queries:**
  - Profit by Year
  - Profit by Year + Product Category
  - Profit by Customer
  - Profit by Customer + Year

---

## ğŸ›¡ï¸ Data Quality & Error Handling

### Implemented Checks:
1. **Schema Validation:** Ensures correct data types and required columns
2. **Null Handling:** Identifies and handles missing values
3. **Duplicate Detection:** Removes duplicate records
4. **Data Type Conversion:** Proper casting of numeric and date fields
5. **Business Rule Validation:** Profit calculation validation

### Error Handling:
- Try-catch blocks for file operations
- Logging of errors and warnings
- Graceful degradation for missing data
- Transaction rollback on failures

---

## ğŸ“ Assumptions & Design Decisions

1. **Date Format:** Order dates assumed to be in ISO format (YYYY-MM-DD)
2. **Deduplication:** Based on primary keys (order_id, customer_id, product_id)
3. **Year Extraction:** From order_date field
4. **Schema Evolution:** Enabled for all Delta tables

---


## ğŸ“ Support & Contact

For questions or issues:
- Create an issue in the project repository
- Contact: vikash110150@gmail.com
- Documentation: [Databricks Documentation](https://docs.databricks.com/)

---

## ğŸ“„ License

This project is created for educational and assessment purposes.

---

## ğŸ¯ Next Steps

1. **Monitor Pipeline:** Set up job monitoring and alerting
2. **Add More Tests:** Increase test coverage to 90%+
3. **Implement CI/CD:** Automate testing and deployment
4. **Add Data Quality Checks:** More comprehensive validation
5. **Performance Tuning:** Optimize for larger datasets
6. **Documentation:** Add more inline code comments

---

**Happy Data Engineering!
